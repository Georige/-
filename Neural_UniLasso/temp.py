import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns
from tqdm import tqdm
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LassoCV, LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.preprocessing import StandardScaler, SplineTransformer
from sklearn.pipeline import make_pipeline
from sklearn.metrics import mean_squared_error, f1_score, recall_score, precision_score

# ==========================================
# 0. å®éªŒé…ç½®è¡¨ (Experiment Configuration)
# ==========================================
CONFIG = {
    # -- ç¡¬ä»¶æ§åˆ¶ --
    "device": "cuda" if torch.cuda.is_available() else ("mps" if torch.backends.mps.is_available() else "cpu"),
    
    # -- æ•°æ®ç”Ÿæˆ --
    "n_train": 1000,
    "n_test": 2000,
    "p_features": 200,
    "data_range": (-3, 3),    # U(-3, 3)
    "noise_std": 0.5,         # å™ªå£°æ°´å¹³
    
    # -- ç¥ç»ç½‘ç»œæ¶æ„ --
    "hidden_structure": [16, 8], 
    "activation": "ELU",
    
    # -- è®­ç»ƒå‚æ•° --
    "epochs": 1500,
    "lr": 0.01,
    "batch_size": 256,       # ä½¿ç”¨ Batch è®­ç»ƒåŠ é€Ÿ
    "l1_lambda": 0.05,       # ç¨€ç–æƒ©ç½šç³»æ•°
    
    # -- ç»˜å›¾å‚æ•° --
    "vis_x_range": (-4, 4),  # å½¢çŠ¶å¯è§†åŒ–èŒƒå›´ï¼ˆæ¯”è®­ç»ƒèŒƒå›´ç¨å¤§ï¼Œçœ‹æ³›åŒ–ï¼‰
    "seed": 42
}

# å…¨å±€è®¾ç½®
torch.manual_seed(CONFIG["seed"])
np.random.seed(CONFIG["seed"])
plt.rcParams['font.sans-serif'] = ['Arial', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False
sns.set_palette("husl")
DEVICE = torch.device(CONFIG["device"])

print(f"ğŸš€ Experiment running on device: {DEVICE}")

# ==========================================
# 1. æ ¸å¿ƒæ¨¡å‹ï¼šNeural UniLasso (GPU Optimized)
# ==========================================
class UnivariateNetwork(nn.Module):
    def __init__(self):
        super().__init__()
        h1, h2 = CONFIG["hidden_structure"]
        self.net = nn.Sequential(
            nn.Linear(1, h1),
            nn.ELU(),
            nn.Linear(h1, h2),
            nn.ELU(),
            nn.Linear(h2, 1)
        )
    def forward(self, x):
        return self.net(x)

class NeuralUniLasso(nn.Module):
    def __init__(self, n_features):
        super().__init__()
        self.n_features = n_features
        # å¹¶è¡Œå­ç½‘ç»œ
        self.uni_nets = nn.ModuleList([UnivariateNetwork() for _ in range(n_features)])
        # èåˆæƒé‡
        self.theta = nn.Parameter(torch.rand(n_features) * 0.05)
        self.bias = nn.Parameter(torch.zeros(1))
        
    def forward(self, x):
        # x: [batch, p]
        z_list = []
        for i in range(self.n_features):
            z_list.append(self.uni_nets[i](x[:, i].view(-1, 1)))
        
        Z = torch.cat(z_list, dim=1) 
        
        # éè´Ÿçº¦æŸ
        self.weights = F.softplus(self.theta)
        
        y_pred = torch.matmul(Z, self.weights) + self.bias
        return y_pred, self.weights, Z

# ==========================================
# 2. æ•°æ®ç”Ÿæˆå™¨ (å«å¤æ‚å‡½æ•°å¤„ç†)
# ==========================================
def generate_data():
    n = CONFIG["n_train"] + CONFIG["n_test"]
    p = CONFIG["p_features"]
    low, high = CONFIG["data_range"]
    
    X = np.random.uniform(low, high, size=(n, p))
    
    # æå–çœŸå®å˜é‡
    x1, x2, x3, x4 = X[:, 0], X[:, 1], X[:, 2], X[:, 3]
    
    # y = sin(x1) - 3*tan(x2) + 5*e^x3 - 2*x4
    # æ³¨æ„ï¼štan(x) åœ¨ (-3,3) å†…æœ‰å¥‡ç‚¹ +/- 1.57ã€‚å¦‚æœä¸æˆªæ–­ï¼Œyä¼šç”±æå¤§å€¼ä¸»å¯¼ã€‚
    t1 = np.sin(x1)
    t2 = -3 * np.clip(np.tan(x2), -10, 10) # æˆªæ–­ tan
    t3 = 5 * np.exp(x3)                    # e^3 ~ 20, e^-3 ~ 0
    t4 = -2 * x4
    
    y_raw = t1 + t2 + t3 + t4
    y = y_raw + np.random.normal(0, CONFIG["noise_std"], n)
    
    true_idx = [0, 1, 2, 3]
    
    return X[:CONFIG["n_train"]], X[CONFIG["n_train"]:], \
           y[:CONFIG["n_train"]], y[CONFIG["n_train"]:], \
           true_idx

# ==========================================
# 3. æ™ºèƒ½é˜ˆå€¼æˆªæ–­ (Smart Cliff Detection)
# ==========================================
def get_cliff_threshold(weights):
    """
    å¯»æ‰¾æƒé‡æ’åºåçš„æœ€å¤§æ–­å´–ï¼Œä½œä¸ºä¿¡å·ä¸å™ªå£°çš„åˆ†ç•Œçº¿ã€‚
    """
    w_abs = np.abs(weights)
    sorted_w = np.sort(w_abs)[::-1] # é™åº
    
    # ä»…åœ¨å¤´éƒ¨åŒºåŸŸæœç´¢ (å‡è®¾çœŸå®å˜é‡æ˜¯å°‘æ•°)
    search_len = min(len(sorted_w)-1, int(len(sorted_w)*0.2) + 5)
    
    # è®¡ç®—ç›¸é‚»è½å·®
    gaps = sorted_w[:search_len] - sorted_w[1:search_len+1]
    
    if len(gaps) == 0: return 0.0
    
    best_gap_idx = np.argmax(gaps)
    
    # é˜ˆå€¼å–æ–­å´–ä¸­é—´
    threshold = (sorted_w[best_gap_idx] + sorted_w[best_gap_idx+1]) / 2
    return threshold

# ==========================================
# 4. æŒ‡æ ‡è®¡ç®—å™¨
# ==========================================
def calc_metrics(y_true, y_pred, selected, true_idx, p):
    mse = mean_squared_error(y_true, y_pred)
    
    y_true_bin = np.zeros(p)
    y_true_bin[true_idx] = 1
    
    y_pred_bin = np.zeros(p)
    y_pred_bin[selected] = 1
    
    # æŒ‡æ ‡
    f1 = f1_score(y_true_bin, y_pred_bin, zero_division=0)
    sens = recall_score(y_true_bin, y_pred_bin, zero_division=0) # Sensitivity/Recall
    
    tn = np.sum((y_true_bin == 0) & (y_pred_bin == 0))
    fp = np.sum((y_true_bin == 0) & (y_pred_bin == 1))
    spec = tn / (tn + fp) if (tn+fp) > 0 else 0.0 # Specificity
    
    return {"MSE": mse, "F1": f1, "Sensitivity": sens, "Specificity": spec, "Count": len(selected)}

# ==========================================
# 5. ä¸»å®éªŒæµç¨‹
# ==========================================
def run_experiment():
    print(f"\n{'='*20} Experiment Start {'='*20}")
    print(f"Config: N={CONFIG['n_train']}, P={CONFIG['p_features']}, Device={DEVICE}")
    
    # 1. æ•°æ®å‡†å¤‡
    X_train, X_test, y_train, y_test, true_idx = generate_data()
    
    # æ ‡å‡†åŒ– (Neural Net éœ€è¦è¾“å…¥æ ‡å‡†åŒ–ï¼ŒGAM/Lasso ä¹Ÿå—ç›Š)
    scaler_X = StandardScaler()
    X_train_s = scaler_X.fit_transform(X_train)
    X_test_s = scaler_X.transform(X_test)
    
    # y ä¹Ÿè¦æ ‡å‡†åŒ–ä»¥ç¨³å®šæ¢¯åº¦ï¼Œä½†åœ¨è¯„ä¼° MSE æ—¶éœ€è¿˜åŸ
    scaler_y = StandardScaler()
    y_train_s = scaler_y.fit_transform(y_train.reshape(-1, 1)).flatten()
    
    # ----------------------------------------------------
    # Model A: Neural UniLasso (GPU)
    # ----------------------------------------------------
    print("\n[Model 1] Training Neural UniLasso...")
    model = NeuralUniLasso(CONFIG["p_features"]).to(DEVICE)
    opt = optim.Adam(model.parameters(), lr=CONFIG["lr"])
    loss_fn = nn.MSELoss()
    
    # è½¬ä¸º GPU Tensor
    Xt = torch.tensor(X_train_s, dtype=torch.float32, device=DEVICE)
    yt = torch.tensor(y_train_s, dtype=torch.float32, device=DEVICE)
    
    # è®­ç»ƒå¾ªç¯ (å¸¦è¿›åº¦æ¡)
    loop = tqdm(range(CONFIG["epochs"]), desc="Training")
    for epoch in loop:
        opt.zero_grad()
        pred, w, _ = model(Xt)
        
        mse = loss_fn(pred.view(-1), yt)
        reg = CONFIG["l1_lambda"] * torch.sum(w)
        loss = mse + reg
        
        loss.backward()
        opt.step()
        
        if epoch % 50 == 0:
            loop.set_postfix(loss=loss.item(), active=(w>1e-3).sum().item())
            
    # æ¨ç†
    model.eval()
    Xt_test = torch.tensor(X_test_s, dtype=torch.float32, device=DEVICE)
    with torch.no_grad():
        _, w_tensor, _ = model(Xt_test)
        w_nu = w_tensor.cpu().numpy()
        pred_scaled = model(Xt_test)[0].cpu().numpy().flatten()
        pred_nu = scaler_y.inverse_transform(pred_scaled.reshape(-1,1)).flatten()
        
    # æ™ºèƒ½æˆªæ–­
    thresh_nu = get_cliff_threshold(w_nu)
    sel_nu = np.where(w_nu > thresh_nu)[0]
    
    # ----------------------------------------------------
    # Model B: Lasso
    # ----------------------------------------------------
    print("[Model 2] Training LassoCV...")
    lasso = LassoCV(cv=5, n_jobs=-1).fit(X_train_s, y_train_s)
    w_lasso = np.abs(lasso.coef_)
    pred_lasso = scaler_y.inverse_transform(lasso.predict(X_test_s).reshape(-1,1)).flatten()
    sel_lasso = np.where(w_lasso > 1e-4)[0]
    
    # ----------------------------------------------------
    # Model C: Random Forest
    # ----------------------------------------------------
    print("[Model 3] Training Random Forest...")
    rf = RandomForestRegressor(n_estimators=100, n_jobs=-1).fit(X_train_s, y_train)
    w_rf = rf.feature_importances_
    pred_rf = rf.predict(X_test_s)
    thresh_rf = get_cliff_threshold(w_rf)
    sel_rf = np.where(w_rf > thresh_rf)[0]
    
    # ----------------------------------------------------
    # Model D: GAMs (Simplified)
    # ----------------------------------------------------
    print("[Model 4] Training GAMs (Splines)...")
    # ä¸ºäº†ç‰¹å¾é€‰æ‹©ï¼Œæˆ‘ä»¬å¯¹ Spline ç³»æ•°è¿›è¡Œç»„ L1 èŒƒæ•°ç­›é€‰æ¯”è¾ƒå›°éš¾
    # è¿™é‡Œæˆ‘ä»¬åªè®¡ç®— GAM çš„é¢„æµ‹ MSE ä½œä¸ºéçº¿æ€§æ¨¡å‹çš„åŸºå‡†
    gam = make_pipeline(SplineTransformer(n_knots=5, degree=3), LinearRegression())
    gam.fit(X_train_s, y_train)
    pred_gam = gam.predict(X_test_s)
    sel_gam = [] # æš‚ä¸å‚ä¸ç‰¹å¾é€‰æ‹©å¯¹æ¯”
    
    # ----------------------------------------------------
    # ç»“æœæ±‡æ€»
    # ----------------------------------------------------
    res = {
        "Neural UniLasso": calc_metrics(y_test, pred_nu, sel_nu, true_idx, CONFIG["p_features"]),
        "Lasso": calc_metrics(y_test, pred_lasso, sel_lasso, true_idx, CONFIG["p_features"]),
        "Random Forest": calc_metrics(y_test, pred_rf, sel_rf, true_idx, CONFIG["p_features"]),
        "GAMs": {"MSE": mean_squared_error(y_test, pred_gam), "F1":0, "Sensitivity":0, "Specificity":0, "Count":0}
    }
    
    df_res = pd.DataFrame(res).T
    print("\n" + "="*40)
    print("EXPERIMENTAL RESULTS")
    print("="*40)
    print(f"Neural Selected: {sel_nu}")
    print(f"Lasso Selected:  {sel_lasso}")
    print(df_res)
    
    # ==========================================
    # 6. å¯è§†åŒ–
    # ==========================================
    plot_visualization(w_nu, w_lasso, w_rf, true_idx, model, scaler_X, scaler_y, df_res)

def plot_visualization(w_nu, w_lasso, w_rf, true_idx, model, scaler_X, scaler_y, df_res):
    fig = plt.figure(figsize=(20, 16))
    plt.suptitle("Neural UniLasso: Feature Learning Benchmark", fontsize=24, y=0.96)
    
    # --- å›¾ä¸€ï¼šç‰¹å¾é€‰æ‹©å¯¹æ¯” ---
    ax1 = plt.subplot(2, 2, 1)
    disp_p = 20
    idx = np.arange(disp_p)
    
    def norm(x): return x/x.max() if x.max()>0 else x
    
    ax1.bar(idx-0.2, norm(w_nu)[:disp_p], 0.2, label='Neural UniLasso', color='#2980b9')
    ax1.bar(idx, norm(w_lasso)[:disp_p], 0.2, label='Lasso', color='#e74c3c', alpha=0.7)
    ax1.bar(idx+0.2, norm(w_rf)[:disp_p], 0.2, label='Random Forest', color='#27ae60', alpha=0.7)
    
    for i in true_idx:
        ax1.axvline(i, color='purple', ls='--', lw=2)
        ax1.text(i, 1.05, f'True X{i+1}', ha='center', color='purple', fontweight='bold')
    
    ax1.set_title("Feature Importance Ranking (Top 20)", fontsize=16)
    ax1.set_xlabel("Feature Index")
    ax1.legend()

    # --- å›¾äºŒï¼šå•å˜é‡å½¢çŠ¶å­¦ä¹  (ä¸å‹ç¼©èŒƒå›´ï¼Œçœ‹çœŸå®æ‹Ÿåˆ) ---
    ax2 = plt.subplot(2, 2, 2)
    
    # ç”Ÿæˆå®½èŒƒå›´æµ‹è¯•æ•°æ® (-4, 4)ï¼Œçœ‹å¤–æ¨èƒ½åŠ›
    x_viz = np.linspace(CONFIG["vis_x_range"][0], CONFIG["vis_x_range"][1], 200)
    # æ„é€ è¾“å…¥ (å¡«å……åˆ°å¯¹åº”åˆ—)
    x_in = np.zeros((200, CONFIG["p_features"]))
    # ç®€å•å‡è®¾æ‰€æœ‰åˆ—å‡å€¼æ–¹å·®ç›¸ä¼¼(å‡åŒ€åˆ†å¸ƒç‰¹æ€§)ï¼Œç›´æ¥ç”¨ fit æ—¶çš„ scaler å˜æ¢
    # å®é™…åº”é’ˆå¯¹æ¯ä¸€åˆ—ï¼Œä½†è¿™é‡Œæ•°æ®åŒåˆ†å¸ƒï¼Œç›´æ¥ç”¨ transform çš„å‚æ•°
    x_in_s = scaler_X.transform(x_in) 
    # æ›¿æ¢å‰å‡ åˆ—ä¸ºæˆ‘ä»¬çš„ x_viz (æ ‡å‡†åŒ–åçš„)
    x_viz_s = (x_viz - scaler_X.mean_[0]) / scaler_X.scale_[0]
    
    colors = ['#d35400', '#8e44ad', '#2c3e50', '#16a085']
    labels = ["sin(x1)", "-3*tan(x2)", "5*e^x3", "-2*x4"]
    
    for i, feat_id in enumerate(true_idx):
        # 1. ç¥ç»ç½‘ç»œè¾“å‡º
        xt = torch.tensor(x_viz_s, dtype=torch.float32, device=DEVICE).view(-1, 1)
        with torch.no_grad():
            z = model.uni_nets[feat_id](xt).cpu().numpy().flatten()
            # æœ‰æ•ˆè´¡çŒ® = z * theta (å°šæœªåå½’ä¸€åŒ–)
            eff_contrib_s = z * w_nu[feat_id]
            # åå½’ä¸€åŒ–åˆ° y çš„åŸå§‹å°ºåº¦
            eff_contrib = eff_contrib_s * scaler_y.scale_[0] 
            # å¿½ç•¥ biasï¼Œå› ä¸ºå½¢çŠ¶ä¸»è¦ç”±æƒé‡å†³å®šï¼Œbias æ˜¯å…¨å±€çš„
            
        # 2. çœŸå®å‡½æ•°
        if feat_id == 0: y_true = np.sin(x_viz)
        elif feat_id == 1: y_true = -3 * np.tan(x_viz) # ç”»å›¾æ—¶ä¸æˆªæ–­ï¼Œçœ‹åŒºåˆ«ï¼Œæˆ–è€…æˆªæ–­
        elif feat_id == 2: y_true = 5 * np.exp(x_viz)
        elif feat_id == 3: y_true = -2 * x_viz
        
        # å¤„ç† y_true çš„ infinite (ä¸ºäº†ç”»å›¾ç¾è§‚)
        y_true = np.clip(y_true, -50, 50)
        
        ax2.plot(x_viz, eff_contrib, color=colors[i], lw=3, label=f'Learned X{feat_id+1}')
        ax2.plot(x_viz, y_true, color=colors[i], ls=':', lw=1.5, alpha=0.6)
    
    ax2.set_title("Learned Function Shapes (Effective Contribution)", fontsize=16)
    ax2.set_xlabel("Input X (Original Scale)")
    ax2.set_ylabel("Contribution to Y")
    ax2.set_ylim(-30, 30) # é™åˆ¶ Y è½´çœ‹æ¸…ç»†èŠ‚
    ax2.legend()
    ax2.grid(True, alpha=0.3)

    # --- å›¾ä¸‰ï¼šæŒ‡æ ‡é›·è¾¾/æŸ±çŠ¶å›¾ ---
    ax3 = plt.subplot(2, 1, 2)
    df_plot = df_res.drop("GAMs") # GAM æ— åˆ†ç±»æŒ‡æ ‡
    x = np.arange(len(df_plot))
    w = 0.2
    
    ax3.bar(x-w, df_plot["F1"], w, label='F1 Score', color='#f1c40f')
    ax3.bar(x, df_plot["Sensitivity"], w, label='Sensitivity', color='#e67e22')
    ax3.bar(x+w, df_plot["Specificity"], w, label='Specificity', color='#95a5a6')
    
    ax3.set_xticks(x)
    ax3.set_xticklabels(df_plot.index, fontsize=12)
    ax3.set_ylim(0, 1.1)
    ax3.set_title("Performance Metrics", fontsize=16)
    ax3.legend(loc='upper left')
    
    # åŒè½´ç”» MSE
    ax4 = ax3.twinx()
    ax4.plot(x, df_plot["MSE"], color='#2c3e50', marker='D', ms=10, lw=2, label='MSE')
    ax4.set_ylabel("MSE (Lower is Better)")
    ax4.legend(loc='upper right')

    plt.tight_layout()
    plt.show()

if __name__ == "__main__":
    run_experiment()
