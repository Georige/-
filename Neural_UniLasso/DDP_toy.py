import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import cvxpy as cp
from cvxpylayers.torch import CvxpyLayer
import matplotlib.pyplot as plt

# ==========================================
# 1. åˆ¶é€ â€œç©å…·â€æ•°æ® (Toy Data Generation)
# ==========================================
def get_toy_data(n_samples=500, n_features=10, valid_features=[0, 1, 2], noise_std=0.1):
    """
    åˆ¶é€ ä¸€ä¸ªå›å½’æ•°æ®é›†ã€‚
    åªæœ‰ valid_features é‡Œçš„å˜é‡æ˜¯æœ‰ç”¨çš„ï¼Œå…¶ä»–çš„éƒ½æ˜¯å™ªéŸ³ã€‚
    å…³ç³»æ˜¯éçº¿æ€§çš„ï¼šy = 2*sin(x0) + 3*x1^2 - 1.5*x2 + noise
    """
    np.random.seed(42)
    X = np.random.randn(n_samples, n_features).astype(np.float32)
    
    # æ„é€ çœŸå®æ ‡ç­¾
    # å˜é‡0: æ­£å¼¦å…³ç³»
    # å˜é‡1: äºŒæ¬¡å…³ç³»
    # å˜é‡2: çº¿æ€§å…³ç³»
    # å˜é‡3-9: çº¯å™ªéŸ³
    y = (2.0 * np.sin(X[:, 0]) + 
         3.0 * (X[:, 1] ** 2) - 
         1.5 * X[:, 2] + 
         np.random.normal(0, noise_std, n_samples)).astype(np.float32)
    
    return torch.from_numpy(X), torch.from_numpy(y).unsqueeze(1)

# ==========================================
# 2. å®šä¹‰å¾®åˆ†ä¼˜åŒ–ç¥ç»ç½‘ç»œ (The Model)
# ==========================================
class DifferentiableLassoSelector(nn.Module):
    def __init__(self, num_features, hidden_dim=32, alpha=0.1):
        super().__init__()
        self.num_features = num_features
        self.alpha = alpha
        
        # å®šä¹‰ç‰¹å¾æå–å™¨ f_i (æ¯ä¸ªå˜é‡ä¸€ä¸ªç‹¬ç«‹çš„å°ç½‘ç»œ)
        self.feature_nets = nn.ModuleList([
            nn.Sequential(
                nn.Linear(1, hidden_dim),
                nn.Tanh(),  # Tanh æ¯”è¾ƒé€‚åˆè¿™ç§ç®€å•çš„éçº¿æ€§æ‹Ÿåˆ
                nn.Linear(hidden_dim, 1) 
            ) for _ in range(num_features)
        ])
        
        # === æ ¸å¿ƒä¿®æ”¹ï¼šæ„å»ºç¬¦åˆ DPP è§„èŒƒçš„ä¼˜åŒ–å±‚ ===
        self.lasso_layer = self._build_dpp_layer(num_features)

    def _build_dpp_layer(self, n):
        lambda_var = cp.Variable(n)
        
        # ã€ä¿®æ”¹ç‚¹1ã€‘è¿™é‡Œå®šä¹‰ L å‚æ•° (Choleskyå› å­)ï¼Œè€Œä¸æ˜¯ Q
        # åªè¦æ¶‰åŠå‚æ•°ç›¸ä¹˜ï¼Œå¿…é¡»éå¸¸å°å¿ƒã€‚sum_squares(Affine) æ˜¯æœ€ç¨³çš„å†™æ³•ã€‚
        L_param = cp.Parameter((n, n)) 
        p_param = cp.Parameter(n)
        
        # ã€ä¿®æ”¹ç‚¹2ã€‘ç›®æ ‡å‡½æ•°æ”¹å†™
        # åŸç†: lambda^T * Q * lambda = || L^T * lambda ||^2
        # cp.sum_squares ä¿è¯äº†å‡¸æ€§ï¼Œæ£€æŸ¥å‘˜(cvxpy)ä¸ä¼šæŠ¥é”™
        objective = cp.Minimize(0.5 * cp.sum_squares(L_param.T @ lambda_var) + p_param.T @ lambda_var)
        
        constraints = [lambda_var >= 0]
        
        problem = cp.Problem(objective, constraints)
        # æ³¨æ„ï¼šä¸€å®šè¦æŠŠ problem å£°æ˜æ¸…æ¥šå†ä¼ è¿›å»
        assert problem.is_dpp(), "è¿™ä¸ªå®šä¹‰å¦‚æœä¸ç¬¦åˆ DPPï¼Œå°±ä¼šåœ¨è¿™é‡ŒæŠ¥é”™ï¼"
        
        return CvxpyLayer(problem, parameters=[L_param, p_param], variables=[lambda_var])

    def forward(self, x, y):
        # x: (Batch, N), y: (Batch, 1)
        batch_size = x.shape[0]
        
        # Step 1: æå–ç‰¹å¾ Z
        features = []
        for i in range(self.num_features):
            xi = x[:, i:i+1]
            fi = self.feature_nets[i](xi)
            features.append(fi)
        Z = torch.cat(features, dim=1) 
        
        # Step 2: å‡†å¤‡å‚æ•°
        # è®¡ç®— Q = Z^T * Z
        Q = torch.matmul(Z.t(), Z)
        
        # ã€å…³é”®ã€‘åŠ ä¸€ç‚¹ç‚¹æŠ–åŠ¨ (Jitter)ï¼Œé˜²æ­¢çŸ©é˜µå¥‡å¼‚å¯¼è‡´ Cholesky å¤±è´¥
        # åœ¨æ•°å€¼è®¡ç®—ä¸­ï¼Œè¿™æ˜¯ä¿è¯ç¨‹åºä¸å´©çš„å¸¸ç”¨æŠ€å·§
        jitter = 1e-4 * torch.eye(self.num_features, device=x.device)
        Q_stable = Q + jitter
        
        # ã€ä¿®æ”¹ç‚¹3ã€‘æ‰‹åŠ¨è®¡ç®— Cholesky åˆ†è§£: Q = L * L^T
        # L æ˜¯ä¸‹ä¸‰è§’çŸ©é˜µ
        try:
            L = torch.linalg.cholesky(Q_stable)
        except RuntimeError:
            # ä¸‡ä¸€è¿˜æ˜¯å¤±è´¥äº†ï¼ˆæå°‘æƒ…å†µï¼‰ï¼Œå›é€€åˆ°ç‰¹å¾å€¼åˆ†è§£æˆ–è€…åŠ å¤§ jitter
            # è¿™é‡Œä¸ºäº†ç®€å•ï¼Œæˆ‘ä»¬ç›´æ¥åŠ å¤§æŠ–åŠ¨é‡è¯•
            L = torch.linalg.cholesky(Q + 1e-2 * torch.eye(self.num_features, device=x.device))
        
        # è®¡ç®— p = alpha * 1 - Z^T * y
        # æ³¨æ„æ ¹æ® batch size ç¼©æ”¾ alphaï¼Œä¿æŒæ¢¯åº¦é‡çº§ä¸€è‡´
        scaled_alpha = self.alpha * batch_size 
        p = scaled_alpha * torch.ones(self.num_features, device=x.device) - torch.matmul(Z.t(), y).squeeze()
        
        # Step 3: è°ƒç”¨ä¼˜åŒ–å±‚ (ä¼ å…¥ L å’Œ p)
        # è¿™ä¸€æ­¥ cvxpylayers ä¼šè‡ªåŠ¨å¤„ç†åå‘ä¼ æ’­
        lambda_star = self.lasso_layer(L, p)[0]
        
        # Step 4: é¢„æµ‹
        y_hat = torch.matmul(Z, lambda_star)
        
        return y_hat, lambda_star

# ==========================================
# 3. è®­ç»ƒä¸å¯è§†åŒ– (Training & Visualization)
# ==========================================
def run_toy_experiment():
    # === å®éªŒé…ç½® ===
    NUM_FEATURES = 10
    VALID_IDX = [0, 1, 2] # çœŸå®æœ‰æ•ˆçš„å˜é‡ç´¢å¼•
    LR = 0.01
    EPOCHS = 60           # è·‘60è½®å·®ä¸å¤šå°±èƒ½çœ‹å‡ºæ¥äº†
    BATCH_SIZE = 100
    ALPHA = 0.5           # Lasso æƒ©ç½šåŠ›åº¦ï¼Œè¶Šå¤§ç¨€ç–æ€§è¶Šå¼º
    
    # === å‡†å¤‡æ•°æ® ===
    X, y = get_toy_data(n_samples=500, n_features=NUM_FEATURES, valid_features=VALID_IDX)
    dataset = torch.utils.data.TensorDataset(X, y)
    dataloader = torch.utils.data.DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)
    
    # === åˆå§‹åŒ–æ¨¡å‹ ===
    model = DifferentiableLassoSelector(num_features=NUM_FEATURES, alpha=ALPHA)
    optimizer = optim.Adam(model.parameters(), lr=LR)
    loss_fn = nn.MSELoss()
    
    # è®°å½•æ•°æ®ç”¨äºç”»å›¾
    history_lambda = [] 
    history_loss = []
    
    print(f"ğŸš€ å¼€å§‹è®­ç»ƒ! çœŸå®æœ‰æ•ˆå˜é‡æ˜¯: {VALID_IDX}")
    print(f"ğŸ”¥ ç›®æ ‡: çœ‹ç€çº¢çº¿(æœ‰æ•ˆå˜é‡)å‡èµ·ï¼Œç°çº¿(å™ªéŸ³)å½’é›¶...")
    print("-" * 60)
    
    for epoch in range(EPOCHS):
        epoch_loss = 0
        epoch_lambdas = []
        
        for batch_X, batch_y in dataloader:
            optimizer.zero_grad()
            
            # Forward (è‡ªåŠ¨è§£ä¼˜åŒ–é—®é¢˜)
            y_pred, lambda_star = model(batch_X, batch_y)
            
            # Loss
            loss = loss_fn(y_pred.unsqueeze(1), batch_y)
            
            # Backward (æ¢¯åº¦ç©¿è¿‡ä¼˜åŒ–å±‚æ›´æ–°ç½‘ç»œ)
            loss.backward()
            optimizer.step()
            
            epoch_loss += loss.item()
            epoch_lambdas.append(lambda_star.detach().numpy())
            
        # è®°å½•ç»Ÿè®¡
        avg_lambda = np.mean(epoch_lambdas, axis=0)
        history_lambda.append(avg_lambda)
        history_loss.append(epoch_loss / len(dataloader))
        
        if (epoch+1) % 10 == 0:
            top_indices = np.argsort(-avg_lambda)[:5]
            print(f"Epoch {epoch+1}/{EPOCHS} | Loss: {epoch_loss:.4f}")
            print(f"   -> Top Weights: ç´¢å¼•{top_indices} (å€¼: {avg_lambda[top_indices].round(2)})")

    # ==========================================
    # 4. ç»˜å›¾ (Visualization)
    # ==========================================
    history_lambda = np.array(history_lambda)
    
    plt.figure(figsize=(14, 6))
    
    # å·¦å›¾: Loss
    plt.subplot(1, 2, 1)
    plt.plot(history_loss, label='MSE Loss', color='black', linewidth=2)
    plt.title("Training Loss", fontsize=14)
    plt.xlabel("Epoch")
    plt.ylabel("Loss")
    plt.grid(True, alpha=0.3)
    
    # å³å›¾: å˜é‡æƒé‡æ¼”å˜
    plt.subplot(1, 2, 2)
    for i in range(NUM_FEATURES):
        if i in VALID_IDX:
            label = f"Feature {i} (Valid)"
            color = 'tab:red'
            alpha = 1.0
            linewidth = 3.0
            linestyle = '-'
        else:
            label = f"Feature {i} (Noise)" if i == 3 else None # åªæ ‡ä¸€ä¸ªlabelé¿å…å›¾ä¾‹å¤ªä¹±
            color = 'gray'
            alpha = 0.3
            linewidth = 1.0
            linestyle = '--'
        
        plt.plot(history_lambda[:, i], label=label, 
                 color=color, alpha=alpha, linewidth=linewidth, linestyle=linestyle)
        
    plt.title("Evolution of Feature Weights (Lambda)", fontsize=14)
    plt.xlabel("Epoch")
    plt.ylabel("Lasso Weight")
    plt.legend(loc='upper left')
    plt.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.show()
    
    print("\nâœ… å®éªŒæˆåŠŸå®Œæˆï¼")

if __name__ == "__main__":
    run_toy_experiment()