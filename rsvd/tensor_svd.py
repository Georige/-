from matplotlib import pyplot as plt
import numpy as np
import torch
import torch.nn as nn
import torch.fft
from dataclasses import dataclass

# ==========================================
# 1. ç³»ç»Ÿé…ç½®
# ==========================================
@dataclass
class Config:
    tensor_inner: int = 10       # å†…éƒ¨è¿­ä»£æ¬¡æ•°
    lamb: float = 0.0005         # åŸºç¡€æ­£åˆ™åŒ–ç³»æ•° lambda
    p: float = 0.75              # Lp èŒƒæ•° (0 < p <= 1)
    epsilon: float = 1e-10       # é˜²æ­¢é™¤é›¶çš„å¾®å°é‡

# ==========================================
# 2. æ ¸å¿ƒæ•°å­¦ç®—å­ (Lp æœ€å°åŒ–æ±‚è§£å™¨)
# ==========================================
def solve_lp_w(y: torch.Tensor, weight_vec: torch.Tensor, p: float, inner_iter: int = 4) -> torch.Tensor:
    """
    æ±‚è§£åŠ æƒ Lp èŒƒæ•°æœ€å°åŒ–é—®é¢˜çš„è¿‘ç«¯ç®—å­ã€‚
    """
    # 1. è®¡ç®—è‡ªé€‚åº”é˜ˆå€¼ tau
    a = 2 * weight_vec * (1 - p)
    power_factor = 1 / (2 - p)
    
    # weight_vec å·²ç»æ˜¯ lambda * w
    tau = torch.pow(a, power_factor) + p * weight_vec * torch.pow(a, (p - 1) * power_factor)
    
    # 2. æ©ç ç­›é€‰
    mask = torch.abs(y) > tau
    x = torch.zeros_like(y)
    
    if not mask.any():
        return x
    
    # 3. å¹¿ä¹‰è½¯é˜ˆå€¼è¿­ä»£ (GST)
    y0 = y[mask]
    lambda0 = weight_vec[mask]
    t = torch.abs(y0)
    
    for _ in range(inner_iter):
        t = torch.abs(y0) - p * lambda0 * torch.pow(t, p - 1)
    
    x[mask] = torch.sign(y0) * t
    return x

# ==========================================
# 3. ä¸»ç®—æ³•æµç¨‹ (é’ˆå¯¹ Batch ä¸ºå˜æ¢ç»´)
# ==========================================
def tensor_log_sp(Y: torch.Tensor, lambdai: float, par: Config) -> torch.Tensor:
    """
    è¾“å…¥å½¢çŠ¶: (Batch, Length, Width)
    é€»è¾‘:
      - å°† Batch (dim 0) è§†ä¸ºå˜æ¢ç»´åº¦ (Transform Dim / Tube)
      - å°† (Length, Width) è§†ä¸ºçŸ©é˜µåˆ‡ç‰‡
    """
    # Y shape: (B, L, W)
    B, L, W = Y.shape
    
    # -----------------------------------------------------------
    # Step 1: FFT å˜æ¢ (æ²¿ç€ Batch ç»´åº¦)
    # -----------------------------------------------------------
    # ç›´æ¥æ²¿ç€ dim=0 è¿›è¡Œ FFT
    # Y_f shape: (B, L, W) - å¤æ•°å¼ é‡
    Y_f = torch.fft.fft(Y, dim=0)
    
    # æ•°æ®æ¸…æ´—
    Y_f = torch.nan_to_num(Y_f, nan=0.0, posinf=0.0, neginf=0.0)

    # -----------------------------------------------------------
    # Step 2: å¹¶è¡Œ SVD åˆ†è§£
    # -----------------------------------------------------------
    # PyTorch çš„ svd é»˜è®¤å¤„ç†æœ€åä¸¤ä¸ªç»´åº¦ä½œä¸ºçŸ©é˜µï¼Œå‰é¢çš„ç»´åº¦ä½œä¸º Batchã€‚
    # æˆ‘ä»¬çš„å½¢çŠ¶æ˜¯ (B, L, W)ï¼Œè¿™å®Œç¾ç¬¦åˆè¦æ±‚ï¼
    # å«ä¹‰ï¼šå¯¹é¢‘åŸŸä¸­çš„æ¯ä¸€ä¸ª 'B' (é¢‘ç‡ç‚¹)ï¼Œåˆ†è§£å…¶ (L, W) çŸ©é˜µã€‚
    
    # U_f: (B, L, K), S_f: (B, K), Vh_f: (B, K, W)
    # å…¶ä¸­ K = min(L, W)
    U_f, S_f, Vh_f = torch.linalg.svd(Y_f, full_matrices=False)
    
    # -----------------------------------------------------------
    # Step 3: è¿­ä»£åŠ æƒæ”¶ç¼©
    # -----------------------------------------------------------
    # S_f æ˜¯å¥‡å¼‚å€¼ (å®æ•°)
    w = 1.0 / (torch.pow(S_f, par.p) + par.epsilon)
    s1 = torch.zeros_like(S_f)
    
    for _ in range(par.tensor_inner):
        w_vec = lambdai * w
        # è°ƒç”¨æ±‚è§£å™¨ (å¹¿æ’­æœºåˆ¶ä¼šè‡ªåŠ¨å¤„ç† Batch ç»´)
        s1 = solve_lp_w(S_f, w_vec, par.p)
        # æ›´æ–°æƒé‡
        w = 1.0 / (torch.pow(s1, par.p) + par.epsilon)

    # -----------------------------------------------------------
    # Step 4: é¢‘åŸŸé‡æ„
    # -----------------------------------------------------------
    # æ„é€ å¯¹è§’çŸ©é˜µ: (B, K) -> (B, K, K)
    S_diag = torch.diag_embed(s1)
    
    # [å…³é”®ä¿®å¤]: ç±»å‹åŒ¹é…
    # å°†å®æ•°å¯¹è§’é˜µè½¬ä¸ºå¤æ•°ï¼Œä»¥ä¾¿ä¸ U_f (å¤æ•°) ç›¸ä¹˜
    S_diag = S_diag.to(U_f.dtype)
    
    # çŸ©é˜µä¹˜æ³•: (B, L, K) @ (B, K, K) @ (B, K, W) -> (B, L, W)
    # è¿™é‡Œä¸éœ€è¦ permuteï¼Œå› ä¸ºç»´åº¦é¡ºåºå·²ç»æ˜¯æ­£ç¡®çš„
    X_f = U_f @ S_diag @ Vh_f
    
    # -----------------------------------------------------------
    # Step 5: é€†å˜æ¢
    # -----------------------------------------------------------
    # æ²¿ç€ Batch ç»´åº¦ (dim=0) é€† FFT
    TensorX = torch.fft.ifft(X_f, dim=0)
    
    # å–å®éƒ¨
    X = torch.real(TensorX)
    
    return X

# ==========================================
# 2. å®éªŒå·¥å…·å‡½æ•°
# ==========================================
def generate_low_rank_data(batch, length, width, rank=5):
    """
    ç”Ÿæˆä¸€ä¸ªäººé€ çš„ä½ç§©å¼ é‡ã€‚
    åŸç†ï¼šé€šè¿‡ä¸¤ä¸ªå°çŸ©é˜µç›¸ä¹˜ (L, r) * (r, W) ç”Ÿæˆç§©ä¸º r çš„çŸ©é˜µï¼Œå¹¶æ‰©å±•åˆ° Batchã€‚
    """
    torch.manual_seed(1024) # å›ºå®šéšæœºç§å­ä»¥ä¾¿å¤ç°
    
    # æˆ‘ä»¬ç”Ÿæˆéš Batch ç¼“æ…¢å˜åŒ–çš„ä½ç§©çŸ©é˜µï¼Œæ¨¡æ‹Ÿè§†é¢‘æˆ–æ—¶é—´åºåˆ—
    data = []
    # åŸºç¡€çŸ©é˜µ
    U = torch.randn(length, rank)
    V = torch.randn(rank, width)
    
    for i in range(batch):
        # å¯¹æ¯ä¸€å¸§åŠ ä¸€ç‚¹å¾®å°çš„æ‰°åŠ¨ï¼Œä¿æŒæ•´ä½“ç›¸å…³æ€§ï¼Œä½†åˆä¸å®Œå…¨ç›¸åŒ
        Ui = U + 0.1 * torch.randn_like(U) * np.sin(i/5.0)
        Vi = V + 0.1 * torch.randn_like(V) * np.cos(i/5.0)
        data.append(Ui @ Vi)
        
    return torch.stack(data) # (Batch, Length, Width)

def calc_psnr(clean, recovered):
    """è®¡ç®—å³°å€¼ä¿¡å™ªæ¯” (PSNR)ï¼Œå›¾åƒå¤„ç†æ ‡å‡†æŒ‡æ ‡"""
    mse = torch.mean((clean - recovered) ** 2)
    if mse == 0: return float('inf')
    max_pixel = clean.max()
    return 20 * torch.log10(max_pixel / torch.sqrt(mse))

# ==========================================
# 3. è¿è¡Œä¸»å®éªŒ
# ==========================================
def run_demo():
    # è®¾ç½®
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ğŸš€ æ­£åœ¨è®¾å¤‡ {device} ä¸Šå¯åŠ¨æµ‹è¯•...\n")
    
    # å‚æ•°
    B, L, W = 32, 64, 64  # æ‰¹æ¬¡(æ—¶é—´), é•¿, å®½
    RANK = 5              # çœŸå®çš„ç§©
    NOISE_LEVEL = 0.4     # å™ªå£°å¼ºåº¦
    
    # 1. åˆ¶é€ æ•°æ®
    print("Step 1: ç”Ÿæˆä½ç§©çœŸå€¼æ•°æ®...")
    clean_tensor = generate_low_rank_data(B, L, W, RANK).to(device)
    
    # 2. æ·»åŠ å™ªå£°
    print("Step 2: æ·»åŠ é«˜æ–¯å™ªå£°...")
    noise = torch.randn_like(clean_tensor) * NOISE_LEVEL * clean_tensor.std()
    noisy_tensor = clean_tensor + noise
    
    # 3. è¿è¡Œç®—æ³•
    print("Step 3: è¿è¡Œ Tensor Log-Sp ç®—æ³• (è¿™å¯èƒ½éœ€è¦å‡ ç§’é’Ÿ)...")
    config = Config(tensor_inner=10, p=0.75, lamb=0.0005) # å‚æ•°é…ç½®
    
    # æ³¨æ„ï¼šlambda éœ€è¦æ ¹æ®å™ªå£°æ°´å¹³å¾®è°ƒï¼Œè¿™é‡Œç»™ä¸€ä¸ªç»éªŒå€¼
    # å™ªå£°è¶Šå¤§ï¼Œlambda åº”è¯¥ç¨å¾®å¤§ä¸€ç‚¹æ¥å¢å¼ºè¿‡æ»¤
    algo_lambda = 0.05 * NOISE_LEVEL 
    
    recovered_tensor = tensor_log_sp(noisy_tensor, algo_lambda, config)
    
    # 4. è¯„ä¼°ç»“æœ
    psnr_noisy = calc_psnr(clean_tensor, noisy_tensor)
    psnr_recovered = calc_psnr(clean_tensor, recovered_tensor)
    
    print("\n" + "="*40)
    print(f"ğŸ“Š å®éªŒç»“æœæŠ¥å‘Š")
    print("="*40)
    print(f"å™ªå£°å›¾åƒ PSNR: {psnr_noisy:.2f} dB (è¶Šä½è¶Šå·®)")
    print(f"æ¢å¤å›¾åƒ PSNR: {psnr_recovered:.2f} dB (è¶Šé«˜è¶Šå¥½)")
    print(f"æå‡å¹…åº¦: +{psnr_recovered - psnr_noisy:.2f} dB")
    print("="*40 + "\n")
    
    # 5. å¯è§†åŒ–ç»˜å›¾
    plot_results(clean_tensor, noisy_tensor, recovered_tensor, idx=B//2)
    analyze_rank_recovery(clean_tensor, noisy_tensor, recovered_tensor)

def plot_results(clean, noisy, recovered, idx):
    """ç»˜åˆ¶å¯¹æ¯”å›¾ï¼šé€‰å– Batch ä¸­çš„æŸä¸€å¸§è¿›è¡Œå±•ç¤º"""
    clean_img = clean[idx].cpu().numpy()
    noisy_img = noisy[idx].cpu().numpy()
    rec_img = recovered[idx].cpu().detach().numpy()
    
    plt.figure(figsize=(15, 5))
    
    # è®¾ç½®ç»Ÿä¸€çš„è‰²é˜¶èŒƒå›´ï¼Œæ–¹ä¾¿å¯¹æ¯”
    vmin, vmax = clean_img.min(), clean_img.max()
    
    plt.subplot(1, 3, 1)
    plt.title("Original (Clean Low-Rank)")
    plt.imshow(clean_img, cmap='viridis', vmin=vmin, vmax=vmax)
    plt.axis('off')
    
    plt.subplot(1, 3, 2)
    plt.title("Corrupted (Input)")
    plt.imshow(noisy_img, cmap='viridis', vmin=vmin, vmax=vmax)
    plt.axis('off')
    
    plt.subplot(1, 3, 3)
    plt.title("Recovered (Algorithm Output)")
    plt.imshow(rec_img, cmap='viridis', vmin=vmin, vmax=vmax)
    plt.axis('off')
    
    plt.tight_layout()
    plt.show()
    print(f"å·²å±•ç¤ºç¬¬ {idx} å¸§çš„åˆ‡ç‰‡å¯¹æ¯”ã€‚")
    

def analyze_rank_recovery(clean, noisy, recovered):
    """
    åˆ†æå¹¶ç»˜åˆ¶å¥‡å¼‚å€¼åˆ†å¸ƒè°± (Singular Value Spectrum)ã€‚
    æˆ‘ä»¬åœ¨é¢‘åŸŸè®¡ç®— SVDï¼Œè¿™æ˜¯ç®—æ³•å®é™…å·¥ä½œçš„åœ°æ–¹ã€‚
    """
    import matplotlib.pyplot as plt
    
    # è¾…åŠ©å‡½æ•°ï¼šè®¡ç®—é¢‘åŸŸå¹³å‡å¥‡å¼‚å€¼
    def get_singular_spectrum(tensor):
        # 1. FFT å˜æ¢ (æ²¿ç€ Batch ç»´åº¦)
        tensor_f = torch.fft.fft(tensor, dim=0)
        
        # 2. SVD åˆ†è§£
        # PyTorch SVD è‡ªåŠ¨å¤„ç†æœ€åä¸¤ç»´ (L, W)
        # S_f shape: (Batch, K) where K = min(L, W)
        _, S_f, _ = torch.linalg.svd(tensor_f, full_matrices=False)
        
        # 3. è®¡ç®—æ‰€æœ‰é¢‘ç‡åˆ‡ç‰‡çš„å¹³å‡å¥‡å¼‚å€¼
        # è¿™ä»£è¡¨äº†å¼ é‡çš„å¹³å‡èƒ½é‡åˆ†å¸ƒ
        mean_singular_values = torch.mean(S_f, dim=0).cpu().detach().numpy()
        
        # å½’ä¸€åŒ– (è®©æœ€å¤§å€¼ä¸º 1ï¼Œæ–¹ä¾¿å¯¹æ¯”å½¢çŠ¶)
        return mean_singular_values / mean_singular_values[0]

    # è·å–ä¸‰ç»„æ•°æ®çš„è°±
    s_clean = get_singular_spectrum(clean)
    s_noisy = get_singular_spectrum(noisy)
    s_rec = get_singular_spectrum(recovered)
    
    # ç»˜å›¾
    plt.figure(figsize=(10, 6))
    x_axis = range(len(s_clean))
    
    # ä½¿ç”¨å¯¹æ•°åæ ‡ï¼Œå› ä¸ºå¥‡å¼‚å€¼ä¸‹é™éå¸¸å¿«ï¼Œå¯¹æ•°è½´èƒ½çœ‹æ¸…ç»†èŠ‚
    plt.semilogy(x_axis, s_clean, 'g-', linewidth=2, label='Ground Truth (Low Rank)')
    plt.semilogy(x_axis, s_noisy, 'r--', linewidth=1.5, alpha=0.6, label='Noisy Input (Long Tail)')
    plt.semilogy(x_axis, s_rec, 'b.-', linewidth=2, label='Recovered (Algorithm)')
    
    plt.title("Singular Value Spectrum Analysis (Log Scale)")
    plt.xlabel("Singular Value Index")
    plt.ylabel("Normalized Magnitude (Log)")
    plt.legend()
    plt.grid(True, which="both", ls="-", alpha=0.5)
    
    # æ ‡æ³¨çœŸå®çš„ç§© (å‡è®¾ç”Ÿæˆæ•°æ®æ—¶ rank=5)
    plt.axvline(x=5, color='k', linestyle=':', label='True Rank Cutoff')
    
    plt.tight_layout()
    plt.show()

if __name__ == "__main__":
    run_demo()
    